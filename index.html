<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>SELF-SUPERVISED KEYPOINT DETECTION with Distilled Depth Keypoint Representation</title>
    <link rel="stylesheet" href="css/general.css">
    <link rel="stylesheet" href="css/citation.css">
    <link rel="stylesheet" href="css/title.css">
</head>

<body>
    <div class="header" id="home" style="padding-bottom: 90px;"></div>

    <section class="title">
      SELF-SUPERVISED KEYPOINT DETECTION WITH DISTILLED DEPTH KEYPOINT REPRESENTATION
    </section>

    <section class="author">
        <affiliation>(Submitted to IEEE ICASSP 2025)</affiliation>
        <p style="margin: 10px 0;">
            <a href="http://amananand.ca/">Aman Anand</a>,
            <a href="https://co-author-website.com/">Elyas Rashno</a>,
            <a href="https://ameskandari.github.io">Amir Eskandari</a>,
            <a href="https://research.cs.queensu.ca/home/farhana/">Farhana Zulkernine</a>
        </p>
        <p style="margin: 10px 0;">
            <a href="https://www.queensu.ca/">Queen's University</a>
        </p>
        <affiliation style="display: block; margin: 10px 0;">School of Computing, Queenâ€™s University, Kingston, K7L 2N8, Canada</affiliation>
        <p style="margin: 10px 0;">
            <a href="https://arxiv.org/pdf/your-paper.pdf" class="links">[paper]</a>
            <a href="https://github.com/your-repo/Auto-Distill" class="links">[code_coming_soon]</a>
        </p>
    </section>

    <div class="container">
        <div class="video-container">
            <video src="asset/demo_keypoints.mp4" type="video/mp4" controls muted autoplay loop class="video-resize">
                Your browser does not support the video tag.
            </video>
        </div>
    </div>

    <div class="header" id="abstract">Abstract</div>
    <div class="line"></div>

    <div class="container">
        <p>
            Existing unsupervised keypoint detection methods apply artificial deformations to images such as masking a significant portion of images and using reconstruction of original image as a learning objective to detect keypoints. However, this approach lacks depth information in the image and often detects keypoints on the background. To address this, we propose Distill-DKP, a novel cross-modal knowledge distillation framework that leverages depth maps and RGB images for keypoint detection in a self-supervised setting. During training, Distill-DKP extracts embedding-level knowledge from a depth-based teacher model to guide an image-based student model with inference restricted to the student model. Experiments show that Distill-DKP significantly outperforms previous unsupervised methods by reducing mean L<sub>2</sub> error by 47.15%  on Human3.6M, mean average error by 5.67%  on Taichi, and improving keypoints accuracy by 1.3% on DeepFashion dataset. Detailed ablation studies demonstrate the sensitivity of knowledge distillation across different layers of the network.
        </p>
    </div>

    <h1 class="header" id="results">Architecture Overview</h1>
    <div class="line"></div>
    <div class="container" style="display: flex; justify-content: center; align-items: center;">
        <iframe src="asset/Distill-DKP_diagram.pdf" width="500" height="600"></iframe>
    </div>



    <h1 class="header" id="results">Animated Illustration</h1>
    <div class="line"></div>
    <div class="container">
        <video src="asset/Distill-DKP.mp4" type="video/mp4" controls muted autoplay loop class="video-resize">
            Your browser does not support the video tag.
        </video>
        <p>
            Auto-Distill operates by distilling knowledge from a depth-based teacher model to an RGB-based student model. The teacher model is pre-trained to detect keypoints on depth maps, which allows it to capture topological information. This knowledge is transferred to the student model, which operates on RGB images. During inference, only the RGB-based student model is used. With the help of depth teachers, students effectively learn to distinguish the foreground and background components and understand the topological structure of objects in the images. This enables the student to detect keypoints accurately, even when the backgrounds have structures. 
        </p>
        
    </div>
    <h1 class="header" id="results">Keypoint Detection on Depth map and Image</h1>
    <div class="line"></div>
    <div class="container">
        <video src="asset/depth_image_demo.mp4" type="video/mp4" controls muted autoplay loop class="video-resize">
            Your browser does not support the video tag.
        </video>
    </div>

    <h1 class="header" id="results">Detected Keypoints Across Datasets</h1>
    <div class="line"></div>
    <div class="container">
        <img src="asset/keypoints.jpg" width=800>
    </div>

    <h1 class="header" id="results">State-of-the-art Comparision</h1>
    <div class="line"></div>
    <div class="container">
        <p>
            Our model shows superior performance over all other unsupervised baselines. The number of keypoints for Human3.6M and DeepFashion datasets is K = 16, and K = 10 for Taichi. Bold and underlined numbers represent the best and the second-best results, respectively. The &#8224; sign represents the results we reproduce by the official code of AutoLink, and the &#42; sign means the thickness-tuned variant of AutoLink. (W/O B) and (WB) mean without background and with background, respectively.
        </p>
        
        <div class="container">
            <img src="asset/Results.png" width=800>
        </div>
    </div>

    <h1 class="header" id="results">Ablation Test on Distillation Sensitivity</h1>
    <div class="line"></div>
    <div class="container">
        <p>
            We conduct detailed ablation tests to understand the sensitivity of KD and the influence of knowledge on different layers of the detector. We vary the loss coefficient (&#947;) from 0.1 to 1 for Human3.6M and Taichi datasets and 0.01 to 0.1 for DeepFashion. We select lower &#947; value due to the simpler background, avoiding model degeneration at higher value of &#947;.
        </p>

        
        <div class="container">
            <img src="asset/ablation_test.png" width=800>
        </div>
    </div>

    <div class="header" id="bibtex">BibTex</div>
    <div class="line"></div>

    <section class="citation">
        available soon
    </section>

</body>

</html>
